# 计算机视觉
## 简介
计算机视觉：自动理解图像和视频中的内容

eg：识别，定位，判断 -> 生成
### 应用
识别静物，人脸识别

自动驾驶：环境感知+决策

图像生成，图片风格迁移，航拍转地图，虚拟主播

视频理解，自动剪辑

## 深度学习算法基础(DL)

### DL发展史
#### 1957:感知机模型
线性分类的二分类模型，输入特征只有两个维度。判断输出时，通过阈值与非线性变换（超过阈值则被激活）

*因此，非线性函数被称为激活函数

单层感知机应用：
* 实现基本逻辑关系（如与或非门在xy平面的投影）
* 无法实现异或（无法使用线性二分类）

#### 多层感知机
通过已有逻辑进行组合，实现异或操作（三个单层感知机的结合）叠加层数的增加使得可以解决更复杂的分类问题。

### 神经网络
选择合适网络结构 -> 找到训练方法，评价性能 -> 找到最优参数

NN的基本单位：
**Neuron神经元**->接受信号，激活，向后输出

**权重weight**：该节点对后面神经元的影响程度

**激活函数**：加权求和后加上一个阈值/偏量值，再通过激活函数，用一种非线性变换控制哪些神经元信息可以继续传递

**一些常用的激活函数**：如单位阶跃函数，Sigmoid函数（将实数映射到0-1），tanh函数（解决了0-center问题），ReLU修正线性单元函数（最常用）

**前馈神经网络**：神经元接受加权求和后接受激活后输出（一般通过矩阵进行加速）

**输出层**：归一化softmax函数，所有的y值和为1（由于求出值是概率），基本上是一个指数平均值

### bp反向算法
Back propagation：反向传播

根据正向传播计算出来的值，和实际标签值的误差比较，返回修正参数

Label：独热矢量，通过得出的结果与label作比较，得出loss->Total loss。

常见损失函数：1/2均方差(MSE),交叉熵(Cross Entropy)

信息熵：信息量与事件概率有关('-log(P(x))')

Gradient Descent:loss function是w b的函数，也是y yh的函数，要求L的最小值，需要求导
，参数沿着导数的反方向更新（不过可能会受到局部最优点的干扰，不过高维时很难遇到局部最优，因为每个维度都要符合，然而平原地带和鞍点是确实存在的）

过程：
* 随机初始化权重
* 计算梯度，分别对两个参数求偏导，更新参数
* 迭代重复

优化器：在下降过程中加入惯性（抵消平原和鞍点的影响）

如何训练？

BGD：批量梯度下降，每次迭代时用所有样本更新梯度

SGD：随机梯度下降，每次迭代一次就更新一次（然而没有很好的用到并行加速，且不准确）

Just a trade off

MBGD:小批量梯度下降，结合二者

Batch:训练样本的分组，Iteration：一个batch训练一次即为一个iter，Epoch：所有训练集训练一次

### CNN卷积神经网络
计算机读取图片时将图片打碎为像素矩阵，每个输入均为颜色码。保持图片的shape（保留空间关系）

why CNN：人类对图像的认知是分层抽象的

作用
* 提取图像特征
* 参数共享，一组卷积核提取相似特性
* 降维，提高运算速率

CNN的卷积层和池化层使图像处理更高效，展开后再连接到全连接网络中应用

卷积核：NN中的神经元，在图片中滑动平移，与覆盖的部分点积得到feature map，可实现降维

彩色图片有3通道，所以卷积核也同样要有深度，因此卷积后变为一张图

多卷积核叠加，多层卷积运算

stride步幅也会影响卷积的速率（即影响重叠部分大小）

padding填充在边缘补零，影响图片的输出尺寸（扩大输出尺寸）

池化层：直接取区域的统计特征